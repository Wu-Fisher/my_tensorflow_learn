{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第六章 用于文本和序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环神经网络和一维卷积神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本向量化：\n",
    "1.文本分割为单词，单词转换为向量\n",
    "2.文本分割为字符，每个字符转为向量\n",
    "3.提取单词n-gram，在转为向量\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单词的one-hot编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = ['The cat sat on the mat.','THe dog ate my homework.']\n",
    "# 注意这里The 和 THe不一样\n",
    "token_index={}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word]= len(token_index)+1\n",
    "            #索引编号从1开始\n",
    "\n",
    "max_length = 10\n",
    "#考虑样本前10个单词\n",
    "print(token_index.values())\n",
    "results = np.zeros(shape=(len(samples),max_length,max(token_index.values())+1))\n",
    "\n",
    "for i , sample in enumerate(samples):\n",
    "    for j , word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index= token_index.get(word)\n",
    "        results[i,j,index]=1.\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 字符级别的onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "samples = ['The cat sat on the mat.','THe dog ate my homework.']\n",
    "characters= string.printable\n",
    "token_index=dict(zip(range(1,len(characters)+1),characters))\n",
    "\n",
    "max_length=50\n",
    "results=np.zeros((len(samples),max_length,max(token_index.keys())+1))\n",
    "for i , sample in enumerate(samples):\n",
    "    for j , character in enumerate(sample):\n",
    "        index= token_index.get(character)\n",
    "        results[i,j,index]=1.\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用keras内置函数实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "#分词器\n",
    "\n",
    "samples = ['The cat sat on the mat.','The dog ate my homework.']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "# 用一千个常用词\n",
    "\n",
    "tokenizer.fit_on_texts(samples)\n",
    "#构建单词索引\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "#字符串转换为证书索引列表\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples,mode='binary')\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.'% len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用散列技巧处理，虽然能够节约空间但是会出现散列冲突"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = ['The cat sat on the mat.','The dog ate my homework.']\n",
    "dimensionality = 1000\n",
    "max_length=10\n",
    "results = np.zeros((len(samples),max_length,dimensionality))\n",
    "for i , sample in enumerate(samples):\n",
    "    for j , word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = abs(hash(word))%dimensionality\n",
    "        results[i,j,index]=1.\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "onehot>稀疏，高维，硬编码\n",
    "词嵌入>密集，低维，从数据学习中得到\n",
    "1.完成主任务的同时学习\n",
    "2.使用预训练好的词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用embdding层\n",
    "对每一个实际任务学习一个新的嵌入空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000,64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将embedding层理解为一个字典，将整数索引映射为（对应词）密集向量\n",
    "输入二维整数张量(samples, sequence_length)\n",
    "返回一个三维浮点数张量，用RNN层或一维卷积层来处理这个三维张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "# 改动调用sequence\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "max_features = 10000\n",
    "maxlen =20\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# print(x_train) 一个整数序列的列表\n",
    "x_train =sequence.pad_sequences(x_train,maxlen=maxlen)\n",
    "x_text =sequence.pad_sequences(x_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 20, 8)             80000     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 160)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 161       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 4s 4ms/step - loss: 0.6699 - acc: 0.6256 - val_loss: 0.6205 - val_acc: 0.6942\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.5426 - acc: 0.7522 - val_loss: 0.5246 - val_acc: 0.7268\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.4603 - acc: 0.7879 - val_loss: 0.5021 - val_acc: 0.7436\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.4207 - acc: 0.8114 - val_loss: 0.4945 - val_acc: 0.7498\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3936 - acc: 0.8248 - val_loss: 0.4934 - val_acc: 0.7516\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3709 - acc: 0.8364 - val_loss: 0.4992 - val_acc: 0.7520\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3497 - acc: 0.8493 - val_loss: 0.5026 - val_acc: 0.7502\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.3299 - acc: 0.8595 - val_loss: 0.5094 - val_acc: 0.7502\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.3108 - acc: 0.8706 - val_loss: 0.5173 - val_acc: 0.7506\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2928 - acc: 0.8799 - val_loss: 0.5253 - val_acc: 0.7484\n",
      "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n",
      "dict_values([[0.6699433326721191, 0.5425626635551453, 0.46031811833381653, 0.42068085074424744, 0.39359456300735474, 0.3708534836769104, 0.34974420070648193, 0.329861044883728, 0.31080755591392517, 0.29283323884010315], [0.6255999803543091, 0.7522000074386597, 0.787850022315979, 0.8113999962806702, 0.8248000144958496, 0.8363500237464905, 0.8492500185966492, 0.8595499992370605, 0.8705999851226807, 0.8799499869346619], [0.6205124258995056, 0.5246308445930481, 0.5021393299102783, 0.49454501271247864, 0.49336859583854675, 0.4992414116859436, 0.5026348829269409, 0.5093768835067749, 0.5172702670097351, 0.5252951383590698], [0.6941999793052673, 0.7268000245094299, 0.7436000108718872, 0.7498000264167786, 0.7516000270843506, 0.7519999742507935, 0.7501999735832214, 0.7501999735832214, 0.7505999803543091, 0.7483999729156494]])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense,Embedding\n",
    "\n",
    "model= Sequential()\n",
    "model.add(Embedding(10000,8,input_length=maxlen))\n",
    "\n",
    "model.add(Flatten())\n",
    "#将输入的三维向量展平为二维\n",
    "\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train,y_train,epochs=10,batch_size=32,validation_split=0.2)\n",
    "\n",
    "print(history.history.keys())\n",
    "print(history.history.values())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1da7e72ad87f70fcd0b6030ed64dbaf661c0a0480be1284c3d24ab4850818859"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
